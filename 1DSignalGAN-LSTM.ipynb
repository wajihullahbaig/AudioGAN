{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1c536e-1467-475b-b496-4f3a72015d03",
   "metadata": {},
   "source": [
    "# Create a 1D signal LSTM GAN for voice recorded signals\n",
    "## Use a very basic GAN architecture that works with a single dimension (1D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ced4fc-361e-4c0a-ae7f-bdf579ae48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427dc01-db52-474a-879f-11abc0f3ed31",
   "metadata": {},
   "source": [
    "# Lets setup some basic configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5b5c63-85bc-4629-a290-6b13e61d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 22050\n",
    "duration = 2  # 2 seconds\n",
    "signal_length = sample_rate * duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9581b-3404-4b8a-9660-55d6716ddc20",
   "metadata": {},
   "source": [
    "## Define the Generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ff2fd89-8f77-44e3-be8c-80eab119a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,signal_length):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, signal_length)\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                input_size=signal_length,\n",
    "                hidden_size=128,\n",
    "                num_layers=1,\n",
    "                bidirectional=True,\n",
    "                batch_first=True,\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm_layer(x)\n",
    "        x = x.view(-1,256)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc3(x)\n",
    "        return x.unsqueeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb858d0-bd76-4dbd-8925-9921343dfec3",
   "metadata": {},
   "source": [
    "## Define the Discriminator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff7129ce-317b-4855-8983-5a9275cef1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,singal_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                input_size=signal_length,\n",
    "                hidden_size=256,\n",
    "                num_layers=1,\n",
    "                bidirectional=True,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 256) \n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm_layer(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b938569-5c74-48f4-ba33-e8728e29c867",
   "metadata": {},
   "source": [
    "## Custom dataset class for voice signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "260d1e41-e99c-42b5-9fc4-bb68d477aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=22050, duration=2):\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]  # Only .npy files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        # Load numpy array\n",
    "        waveform = np.load(file_path)\n",
    "        # Ensure the audio is 2 seconds long\n",
    "        target_length = self.sample_rate * self.duration\n",
    "        if waveform.size < target_length:\n",
    "            # Pad the waveform if it's shorter than 2 seconds\n",
    "            waveform = np.pad(waveform, (0, target_length - waveform.size), mode='constant')\n",
    "        else:\n",
    "            # Trim the waveform if it's longer than 2 seconds\n",
    "            waveform = waveform[:target_length]\n",
    "            waveform = np.int16(waveform / np.max(np.abs(waveform)) * 32767)\n",
    "        return torch.tensor(waveform, dtype=torch.float32)  # Convert to tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3879f-d9ac-473c-9f17-00b28d4fb39b",
   "metadata": {},
   "source": [
    "## Prepare the dataset for training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1a497c63-9f08-48d9-9069-11c63d3d7e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset and dataloader\n",
    "dataset = AudioDataset('audios/', sample_rate, duration)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e873b18-a081-4772-b424-835cc4751744",
   "metadata": {},
   "source": [
    "## Initialize the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7e979f70-b32e-417b-9b77-b09f94515a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GAN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Instantiate the models\n",
    "generator = Generator(signal_length)\n",
    "discriminator = Discriminator(signal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfe567-7e87-4aed-b32e-9442593c11ef",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "02afb570-d1ee-4dfd-be1e-7feb821cae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c1eb9-89da-4ef0-8c8e-d3c49ceffebe",
   "metadata": {},
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1a492-5405-4df7-9a52-87a83b8a2be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Generator Loss: 0.7805198431015015, Discriminator Loss: 1.3861932754516602\n",
      "Epoch: 0, Generator Loss: 0.9865930080413818, Discriminator Loss: 1.2145792245864868\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader: # batch is (batch_size, time_steps, features)\n",
    "        # Generate fake signal\n",
    "        noise = torch.randn(batch.shape[0], batch.shape[1]) \n",
    "        generated_signal = generator(noise)\n",
    "\n",
    "        # Create labels\n",
    "        real_labels = torch.ones(batch.size(0), 1)\n",
    "        fake_labels = torch.zeros(batch.size(0), 1)\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_output = discriminator(batch.squeeze(-1))  # Use the real audio batch\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "        fake_output = discriminator(generated_signal.detach())  # Use the generated signal\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        generator.zero_grad()\n",
    "        fake_output = discriminator(generated_signal.detach())  # Use the generated signal\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    #if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03088c-3b01-4469-86f0-573be660c2ef",
   "metadata": {},
   "source": [
    "## Generate new signals after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab179bd-46b1-47e1-aea2-8b8f954071d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the generator to evaluation mode\n",
    "generator.eval()\n",
    "# Generate and visualize a sample output\n",
    "with torch.no_grad():\n",
    "    sample_noise = torch.randn(1,signal_length)  # Adjust noise shape\n",
    "    generated_audio = generator(sample_noise).detach().numpy().flatten()\n",
    "print(generated_audio.shape)\n",
    "# Plot the generated audio signal\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(generated_audio, label='Generated Audio Signal', alpha=0.5)\n",
    "plt.title('Generated Audio Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a806b2-88ea-4001-8361-fc18b9869dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output\n",
    "np.save('generated/generated.npy', generated_audio)\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Assuming the array values are in the range of int16 for WAV format\n",
    "# Scale the array if necessary\n",
    "generated_audio = np.int16(generated_audio / np.max(np.abs(generated_audio)) * 32767)\n",
    "\n",
    "# Save the array as a WAV file\n",
    "write('generated/generated.wav', sample_rate, generated_audio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dda48-e0a8-46ba-a29f-f616bf24d88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
